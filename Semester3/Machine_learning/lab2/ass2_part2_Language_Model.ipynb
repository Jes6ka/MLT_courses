{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "dirty_sentences = list()\n",
    "\n",
    "files = glob.glob(\"carroll\\\\*.txt\")\n",
    "\n",
    "############### sentence split ###############\n",
    "for file_ in files:\n",
    "\t#utf8 doen't work, utf-8-sig removes BOM ufeff\n",
    "\t#with open(file_, 'r', encoding='utf-8-sig') as f: \n",
    "\twith open(file_, 'r', encoding='utf-8') as f: \n",
    "\t\tdirty_sentences += [line.lower().rstrip('\\n') for line in f if line.rstrip('\\n')!='']\n",
    "\t\t#append is twice fast, but my case, this is simple enough.\n",
    "\n",
    "dirty_sentences = ' '.join(dirty_sentences)\n",
    "############### sentence split ###############\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "documents = punkt_sents = sent_detector.tokenize(dirty_sentences.strip())\n",
    "\n",
    "#print(punkt_sents[:20])\n",
    "\n",
    "############### tokenize ###############\n",
    "#from nltk.parse.stanford import StanfordParser\n",
    "#parser=StanfordParser(model_path=\"D:\\GU_MLT\\Semester_3\\Machine_Learning\\lab2\\stanford-parser-full-2017-06-09\\englishPCFG.ser.gz\")\n",
    "#list(parser.raw_parse(\"the quick brown fox jumps over the lazy dog\"))\n",
    "\n",
    "# lines = ['line1', 'line2']\n",
    "# with open('filename.txt', 'w') as f:\n",
    "#     f.writelines(\"%s\\n\" % l for l in lines)\n",
    "\n",
    "with open('documents_sents.txt', 'w', encoding='utf8') as f:\n",
    "\tf.writelines(\"%s\\n\" % l for l in documents)\n",
    "\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "#from nltk.tokenize.stanford import CoreNLPTokenizer\n",
    "# StanfordTokenizer().tokenize(s)\n",
    "\n",
    "tokenized_sents = list()\n",
    "i=1\n",
    "total = len(punkt_sents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for sent in punkt_sents:\n",
    "\ttemp = [\"<Start>\"]\n",
    "\ttemp += StanfordTokenizer().tokenize(sent)\n",
    "\ttemp.append(\"<End>\")\n",
    "\ttokenized_sents.append(temp)\n",
    "\tprint(\"proceed %d / %d\" %(i, total))\n",
    "\ti+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#pickle.dump(tokenized_sents, open('tokenized_sents.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tokenized_sents = pickle.load(open('tokenized_sents.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_voca length is  69747\n",
      "[['<Start>', 'project', 'gutenberg', \"'s\", 'alice', \"'s\", 'adventures', 'under', 'ground', ',', 'by', 'lewis', 'carroll', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', '<End>'], ['<Start>', 'you', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', 'title', ':', 'alice', \"'s\", 'adventures', 'under', 'ground', 'author', ':', 'lewis', 'carroll', 'release', 'date', ':', 'august', '7', ',', '2006', '-LSB-', 'ebook', '#', '19002', '-RSB-', 'language', ':', 'english', '***', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'alice', \"'s\", 'adventures', 'under', 'ground', '***', 'produced', 'by', 'jason', 'isbell', ',', 'sankar', 'viswanathan', ',', 'and', 'the', 'online', 'distributed', 'proofreading', 'team', 'at', 'http://www.pgdp.net', 'alice', \"'s\", 'adventures', 'under', 'ground', '_', 'being', 'a', 'facsimile', 'of', 'the', '_', '_', 'original', 'ms.', 'book', '_', '_', 'afterwards', 'developed', 'into', '_', \"''\", '_', 'alice', \"'s\", 'adventures', 'in', 'wonderland', '_', \"''\", 'by', 'lewis', 'carroll', '_', 'with', 'thirty-seven', 'illustrations', 'by', 'the', 'author', '_', '_', 'price', 'four', 'shillings', '_', 'london', 'macmillan', 'and', 'co.', '.', 'and', 'new', 'york', '1886', '*', '*', '*', '*', '*', 'contents', '.', '<End>'], ['<Start>', 'chapter', 'i.', 'down', 'the', 'rabbit-hole', '.', '<End>'], ['<Start>', 'the', 'pool', 'of', 'tears', 'ii', '.', '<End>'], ['<Start>', 'a', 'long', 'tale', '.', '<End>']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\narray(word1\\t   [[ 1.,  0.,  0., ...,  0.,  0.,  0.],\\n\\t  word2  \\t[ 0.,  1.,  0., ...,  0.,  0.,  0.],\\n      word3 \\t[ 0.,  0.,  1., ...,  0.,  0.,  0.],\\n       ..., \\n      word[N-2] [ 0.,  0.,  0., ...,  1.,  0.,  0.],\\n      word[N-1] [ 0.,  0.,  0., ...,  0.,  1.,  0.],\\n      word[N] \\t[ 0.,  0.,  0., ...,  0.,  0.,  1.]])\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_voca = [a for b in tokenized_sents for a in b]\n",
    "print(\"flatten_voca length is \", len(flatten_voca))\n",
    "vocabulary = sorted(list(set(flatten_voca)))\n",
    "\n",
    "with open('vocabulary.txt', 'w', encoding='utf8') as f:\n",
    "\tf.writelines(\"%s\\n\" % l for l in vocabulary)\n",
    "\n",
    "\n",
    "print(tokenized_sents[:5])\n",
    "############### trigram by n-gram from nltk ###############\n",
    "#tokenized_sents\n",
    "from nltk import ngrams\n",
    "from sklearn import preprocessing\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# le  = preprocessing.LabelEncoder()\n",
    "ohe = preprocessing.OneHotEncoder() \n",
    "#one-hot representations\n",
    "\n",
    "# le.fit(vocabulary)#\n",
    "\n",
    "onehot = ohe.fit_transform([[i] for i in range(len(vocabulary))]).toarray()\n",
    "#one line : onehot = np_utils.to_categorical(ohe.fit_transform(vocabulary))\n",
    "\"\"\"\n",
    "array(word1\t   [[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
    "\t  word2  \t[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
    "      word3 \t[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
    "       ..., \n",
    "      word[N-2] [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
    "      word[N-1] [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
    "      word[N] \t[ 0.,  0.,  0., ...,  0.,  0.,  1.]])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = pd.DataFrame(onehot, index=vocabulary)\n",
    "\n",
    "#print(onehot.shape)\n",
    "\n",
    "\n",
    "# N-gram\n",
    "onehot_sent  = list()\n",
    "onehot_sents = list()\n",
    "#i =0\n",
    "#print(tokenized_sents[1], len(tokenized_sents[1]))\n",
    "for sent in tokenized_sents:\n",
    "    for word in sent:\n",
    "        onehot_sent.append(onehot.loc[word])\n",
    "        i+=1\n",
    "    #print(len(onehot_sent), len(onehot_sent[0]))\n",
    "    onehot_sents+=list(ngrams(onehot_sent, 3)) # each list has (onehot_word1, onehot_word2, onehot_word3)\n",
    "    onehot_sent.clear()\n",
    "    #print(\"i is \", i, len(onehot_sents), len(onehot_sents[0]), len(onehot_sents[1]))\n",
    "\n",
    "onehot_sents = pd.DataFrame(onehot_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "      <td>0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "1  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "2  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "3  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "4  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "\n",
       "                                                   1  \\\n",
       "0  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "1  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "2  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "3  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "4  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....   \n",
       "\n",
       "                                                   2  \n",
       "0  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....  \n",
       "1  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....  \n",
       "2  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....  \n",
       "3  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....  \n",
       "4  0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_sents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4262,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_sents.iloc[0,0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gc\n",
    "inputs, outputs = list(), list()\n",
    "onehot_sents = list(onehot_sents.as_matrix())\n",
    "row_numb = len(onehot_sents)\n",
    "i,j = 1, 1\n",
    "\n",
    "for n, (first, second, third) in enumerate(onehot_sents):\n",
    "    inputs.append((first, second))\n",
    "    outputs.append(third)\n",
    "    if n == int(row_numb*i/100):\n",
    "        print(i, \"% is done\")\n",
    "        pickle.dump(inputs, open('inputs'+str(i)+'.p', 'wb'))\n",
    "        pickle.dump(outputs, open('outputs'+str(i)+'.p', 'wb'))\n",
    "        inputs.clear()\n",
    "        outputs.clear()\n",
    "        i+=1\n",
    "        \n",
    "    if n == int(row_numb*j/1000):\n",
    "        del [first, second, third]\n",
    "        gc.collect()\n",
    "        j+=1\n",
    "\n",
    "print(len(inputs), len(outputs))\n",
    "print(len(inputs[0][0]),type(inputs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# inputs, outputs = list(), list()\n",
    "# row_numb, col_numb = onehot_sents.shape\n",
    "# i = 1\n",
    "\n",
    "# for index, row in onehot_sents.iterrows():\n",
    "#     inputs.append((list(row[0]), list(row[1])))\n",
    "#     outputs.append(list(row[2]))\n",
    "#     if index == int(row_numb*i/100):\n",
    "#         print(i, \"% is done\")\n",
    "#         i+=1\n",
    "#         pickle.dump(tokenized_sents, open('inputs'+str(i)+'.p', 'wb'))\n",
    "#     del [index, row]\n",
    "#     gc.collect()\n",
    "\n",
    "# print(len(inputs), len(outputs))\n",
    "# print(len(inputs[0][0]),type(inputs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4262"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer is dense with the input dimension of two characters side-by-side in \n",
    "# our 103-character representation. But we compress to 128. \n",
    "# We have sigmoid activation and 10% dropout.\n",
    "# We get a 103-wide output layer to get the third character, and a softmax layer\n",
    "# for the actual prediction.\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(LSTM(256, input_shape=(1, 4262*2), return_sequences=True))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(units=4262))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.add(Dense(units=256, input_dim=4262*2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, input_dim=4262*2))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=4262))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM is a fancier gradient descent optimizer. The average cross-entropy gives us a\n",
    "# perplexity measure.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               2182400   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4262)              1095334   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4262)              0         \n",
      "=================================================================\n",
      "Total params: 3,343,526\n",
      "Trainable params: 3,343,526\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inputs1.p', 'outputs1.p'),\n",
       " ('inputs10.p', 'outputs10.p'),\n",
       " ('inputs11.p', 'outputs11.p'),\n",
       " ('inputs12.p', 'outputs12.p'),\n",
       " ('inputs13.p', 'outputs13.p'),\n",
       " ('inputs14.p', 'outputs14.p'),\n",
       " ('inputs15.p', 'outputs15.p'),\n",
       " ('inputs16.p', 'outputs16.p'),\n",
       " ('inputs17.p', 'outputs17.p'),\n",
       " ('inputs18.p', 'outputs18.p')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inout_list = zip(glob.glob(\"inputs*.p\"), glob.glob(\"outputs*.p\"))\n",
    "inout_list= list(inout_list)\n",
    "inout_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost time : 81.22364568710327 second\n"
     ]
    }
   ],
   "source": [
    "import gc, time\n",
    "inputs = []\n",
    "outputs = []\n",
    "temp = []\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "# 40% inout_list will chock, i5-2.8GHz, 8GB RAM. Gtx840M\n",
    "for input_file, output_file in inout_list[:75]:\n",
    "    temp = pickle.load(open(input_file, 'rb'))\n",
    "    inputs += temp\n",
    "    temp.clear(); gc.collect()\n",
    "    temp = pickle.load(open(output_file, 'rb'))\n",
    "    outputs += temp\n",
    "    temp.clear(); gc.collect()\n",
    "\n",
    "print(\"cost time : %s second\" %(time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4262, 4262)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs), len(outputs) #(13054, 13054)\n",
    "len(inputs[0]), len(outputs[0]) # (2, 4262)\n",
    "len(inputs[0][0]), len(outputs[0]) # (4262, 4262)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='weights80percent_e50_b16.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost time : 156.17893290519714 second\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "longinputs = [list(i[0].as_matrix())+list(i[1].as_matrix()) for i in inputs]\n",
    "print(\"cost time : {} second\".format(time.time() - time_start))\n",
    "#cost time : 372.3062946796417 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44055 samples, validate on 4895 samples\n",
      "Epoch 1/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 5.9249 - acc: 0.0867Epoch 00000: val_loss improved from inf to 5.66172, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 225s - loss: 5.9249 - acc: 0.0867 - val_loss: 5.6617 - val_acc: 0.0981\n",
      "Epoch 2/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 5.5627 - acc: 0.1086Epoch 00001: val_loss improved from 5.66172 to 5.58307, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 152s - loss: 5.5630 - acc: 0.1086 - val_loss: 5.5831 - val_acc: 0.1101\n",
      "Epoch 3/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 5.3794 - acc: 0.1205Epoch 00002: val_loss improved from 5.58307 to 5.53207, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 140s - loss: 5.3800 - acc: 0.1205 - val_loss: 5.5321 - val_acc: 0.1236\n",
      "Epoch 4/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 5.2338 - acc: 0.1326Epoch 00003: val_loss improved from 5.53207 to 5.52354, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 140s - loss: 5.2338 - acc: 0.1326 - val_loss: 5.5235 - val_acc: 0.1406\n",
      "Epoch 5/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 5.0794 - acc: 0.1498Epoch 00004: val_loss improved from 5.52354 to 5.40928, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 140s - loss: 5.0793 - acc: 0.1499 - val_loss: 5.4093 - val_acc: 0.1504\n",
      "Epoch 6/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.9226 - acc: 0.1653Epoch 00005: val_loss improved from 5.40928 to 5.32405, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 140s - loss: 4.9226 - acc: 0.1653 - val_loss: 5.3241 - val_acc: 0.1810\n",
      "Epoch 7/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.7793 - acc: 0.1806Epoch 00006: val_loss improved from 5.32405 to 5.24833, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 139s - loss: 4.7794 - acc: 0.1805 - val_loss: 5.2483 - val_acc: 0.1783\n",
      "Epoch 8/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.6526 - acc: 0.1897Epoch 00007: val_loss improved from 5.24833 to 5.21637, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 139s - loss: 4.6523 - acc: 0.1897 - val_loss: 5.2164 - val_acc: 0.1879\n",
      "Epoch 9/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.5566 - acc: 0.1957Epoch 00008: val_loss improved from 5.21637 to 5.19552, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 140s - loss: 4.5562 - acc: 0.1958 - val_loss: 5.1955 - val_acc: 0.1835\n",
      "Epoch 10/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.4610 - acc: 0.2031Epoch 00009: val_loss improved from 5.19552 to 5.17430, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 140s - loss: 4.4610 - acc: 0.2031 - val_loss: 5.1743 - val_acc: 0.1859\n",
      "Epoch 11/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.3752 - acc: 0.2122Epoch 00010: val_loss improved from 5.17430 to 5.14451, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 140s - loss: 4.3752 - acc: 0.2122 - val_loss: 5.1445 - val_acc: 0.1996\n",
      "Epoch 12/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 4.2951 - acc: 0.2153- ETA: 0s - loss: 4.2957 - acc: 0.2Epoch 00011: val_loss improved from 5.14451 to 5.13877, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 140s - loss: 4.2953 - acc: 0.2153 - val_loss: 5.1388 - val_acc: 0.1996\n",
      "Epoch 13/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.2233 - acc: 0.2215Epoch 00012: val_loss improved from 5.13877 to 5.13875, saving model to weights80percent_e50_b16.hdf5\n",
      "44055/44055 [==============================] - 139s - loss: 4.2234 - acc: 0.2216 - val_loss: 5.1388 - val_acc: 0.1992\n",
      "Epoch 14/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.1481 - acc: 0.2301Epoch 00013: val_loss did not improve\n",
      "44055/44055 [==============================] - 140s - loss: 4.1481 - acc: 0.2301 - val_loss: 5.1599 - val_acc: 0.1873\n",
      "Epoch 15/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.0811 - acc: 0.2352Epoch 00014: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 4.0811 - acc: 0.2352 - val_loss: 5.1550 - val_acc: 0.1957\n",
      "Epoch 16/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 4.0143 - acc: 0.2421Epoch 00015: val_loss did not improve\n",
      "44055/44055 [==============================] - 140s - loss: 4.0143 - acc: 0.2421 - val_loss: 5.1401 - val_acc: 0.2020\n",
      "Epoch 17/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.9486 - acc: 0.2473Epoch 00016: val_loss did not improve\n",
      "44055/44055 [==============================] - 140s - loss: 3.9486 - acc: 0.2473 - val_loss: 5.1608 - val_acc: 0.1992\n",
      "Epoch 18/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.8887 - acc: 0.2537Epoch 00017: val_loss did not improve\n",
      "44055/44055 [==============================] - 140s - loss: 3.8885 - acc: 0.2537 - val_loss: 5.1820 - val_acc: 0.2035\n",
      "Epoch 19/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 3.8264 - acc: 0.2599Epoch 00018: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 3.8263 - acc: 0.2599 - val_loss: 5.1816 - val_acc: 0.1982\n",
      "Epoch 20/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.7686 - acc: 0.2637Epoch 00019: val_loss did not improve\n",
      "44055/44055 [==============================] - 141s - loss: 3.7690 - acc: 0.2637 - val_loss: 5.2075 - val_acc: 0.1996\n",
      "Epoch 21/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.7152 - acc: 0.2696Epoch 00020: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 3.7151 - acc: 0.2696 - val_loss: 5.2146 - val_acc: 0.2033\n",
      "Epoch 22/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 3.6559 - acc: 0.2755Epoch 00021: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 3.6554 - acc: 0.2756 - val_loss: 5.2252 - val_acc: 0.2022\n",
      "Epoch 23/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 3.5993 - acc: 0.2814Epoch 00022: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 3.5992 - acc: 0.2814 - val_loss: 5.2451 - val_acc: 0.2025\n",
      "Epoch 24/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.5419 - acc: 0.2873Epoch 00023: val_loss did not improve\n",
      "44055/44055 [==============================] - 138s - loss: 3.5422 - acc: 0.2872 - val_loss: 5.2534 - val_acc: 0.2067\n",
      "Epoch 25/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.4857 - acc: 0.2922Epoch 00024: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 3.4857 - acc: 0.2922 - val_loss: 5.2978 - val_acc: 0.1975\n",
      "Epoch 26/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 3.4327 - acc: 0.2954Epoch 00025: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 3.4330 - acc: 0.2954 - val_loss: 5.3073 - val_acc: 0.2012\n",
      "Epoch 27/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.3782 - acc: 0.3017Epoch 00026: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 3.3785 - acc: 0.3017 - val_loss: 5.3494 - val_acc: 0.1984\n",
      "Epoch 28/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.3351 - acc: 0.3074Epoch 00027: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 3.3351 - acc: 0.3074 - val_loss: 5.3481 - val_acc: 0.2031\n",
      "Epoch 29/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.2780 - acc: 0.3116Epoch 00028: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 3.2780 - acc: 0.3116 - val_loss: 5.3967 - val_acc: 0.1973\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.2317 - acc: 0.3177Epoch 00029: val_loss did not improve\n",
      "44055/44055 [==============================] - 136s - loss: 3.2317 - acc: 0.3177 - val_loss: 5.3893 - val_acc: 0.2061\n",
      "Epoch 31/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.1777 - acc: 0.3237Epoch 00030: val_loss did not improve\n",
      "44055/44055 [==============================] - 137s - loss: 3.1777 - acc: 0.3237 - val_loss: 5.4415 - val_acc: 0.1992\n",
      "Epoch 32/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.1317 - acc: 0.3276Epoch 00031: val_loss did not improve\n",
      "44055/44055 [==============================] - 137s - loss: 3.1319 - acc: 0.3276 - val_loss: 5.4760 - val_acc: 0.2012\n",
      "Epoch 33/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 3.0854 - acc: 0.3303Epoch 00032: val_loss did not improve\n",
      "44055/44055 [==============================] - 137s - loss: 3.0853 - acc: 0.3303 - val_loss: 5.5061 - val_acc: 0.1965\n",
      "Epoch 34/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 3.0338 - acc: 0.3360- ETA: 1s - Epoch 00033: val_loss did not improve\n",
      "44055/44055 [==============================] - 138s - loss: 3.0341 - acc: 0.3359 - val_loss: 5.5193 - val_acc: 0.1971\n",
      "Epoch 35/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.9882 - acc: 0.3406Epoch 00034: val_loss did not improve\n",
      "44055/44055 [==============================] - 138s - loss: 2.9884 - acc: 0.3406 - val_loss: 5.5507 - val_acc: 0.1992\n",
      "Epoch 36/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.9438 - acc: 0.3470Epoch 00035: val_loss did not improve\n",
      "44055/44055 [==============================] - 138s - loss: 2.9437 - acc: 0.3470 - val_loss: 5.5899 - val_acc: 0.1926\n",
      "Epoch 37/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.8939 - acc: 0.3532Epoch 00036: val_loss did not improve\n",
      "44055/44055 [==============================] - 138s - loss: 2.8939 - acc: 0.3532 - val_loss: 5.5856 - val_acc: 0.1961\n",
      "Epoch 38/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.8526 - acc: 0.3571Epoch 00037: val_loss did not improve\n",
      "44055/44055 [==============================] - 138s - loss: 2.8524 - acc: 0.3572 - val_loss: 5.6264 - val_acc: 0.1945\n",
      "Epoch 39/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.8114 - acc: 0.3619Epoch 00038: val_loss did not improve\n",
      "44055/44055 [==============================] - 138s - loss: 2.8116 - acc: 0.3618 - val_loss: 5.6400 - val_acc: 0.1894\n",
      "Epoch 40/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.7703 - acc: 0.3664Epoch 00039: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 2.7703 - acc: 0.3664 - val_loss: 5.6939 - val_acc: 0.1924\n",
      "Epoch 41/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 2.7246 - acc: 0.3719Epoch 00040: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 2.7244 - acc: 0.3720 - val_loss: 5.7150 - val_acc: 0.1943\n",
      "Epoch 42/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 2.6808 - acc: 0.3778Epoch 00041: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 2.6809 - acc: 0.3777 - val_loss: 5.7554 - val_acc: 0.1896\n",
      "Epoch 43/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.6539 - acc: 0.3808Epoch 00042: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 2.6539 - acc: 0.3808 - val_loss: 5.7652 - val_acc: 0.1847\n",
      "Epoch 44/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 2.6107 - acc: 0.3903Epoch 00043: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 2.6105 - acc: 0.3904 - val_loss: 5.7908 - val_acc: 0.1902\n",
      "Epoch 45/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.5713 - acc: 0.3949Epoch 00044: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 2.5714 - acc: 0.3949 - val_loss: 5.8310 - val_acc: 0.1914\n",
      "Epoch 46/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 2.5415 - acc: 0.3993Epoch 00045: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 2.5413 - acc: 0.3994 - val_loss: 5.8615 - val_acc: 0.1851\n",
      "Epoch 47/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.5029 - acc: 0.4044Epoch 00046: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 2.5029 - acc: 0.4044 - val_loss: 5.8865 - val_acc: 0.1867\n",
      "Epoch 48/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.4693 - acc: 0.4102Epoch 00047: val_loss did not improve\n",
      "44055/44055 [==============================] - 139s - loss: 2.4694 - acc: 0.4101 - val_loss: 5.9145 - val_acc: 0.1812\n",
      "Epoch 49/50\n",
      "44032/44055 [============================>.] - ETA: 0s - loss: 2.4372 - acc: 0.4111Epoch 00048: val_loss did not improve\n",
      "44055/44055 [==============================] - 138s - loss: 2.4373 - acc: 0.4111 - val_loss: 5.9275 - val_acc: 0.1890\n",
      "Epoch 50/50\n",
      "44048/44055 [============================>.] - ETA: 0s - loss: 2.4035 - acc: 0.4180Epoch 00049: val_loss did not improve\n",
      "44055/44055 [==============================] - 138s - loss: 2.4034 - acc: 0.4180 - val_loss: 5.9511 - val_acc: 0.1785\n",
      "cost time : 7536.25204873085 second\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16,\n",
       " 'do_validation': True,\n",
       " 'epochs': 50,\n",
       " 'metrics': ['loss', 'acc', 'val_loss', 'val_acc'],\n",
       " 'samples': 44055,\n",
       " 'steps': None,\n",
       " 'verbose': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "history = model.fit(np.array(longinputs), np.array(outputs), epochs=50, batch_size=16, validation_split=0.1, callbacks=[checkpointer])\n",
    "print(\"cost time : {} second\".format(time.time() - time_start))\n",
    "\n",
    "import h5py\n",
    "model.save('carroll_model80.h5')\n",
    "history.params\n",
    "\n",
    "# 35% data\n",
    "# Epoch 50/50\n",
    "# 20544/20559 [============================>.] - ETA: 0s - loss: 2.2758 - acc: 0.4596Epoch 00049: val_loss did not improve\n",
    "# 20559/20559 [==============================] - 68s - loss: 2.2760 - acc: 0.4595 - val_loss: 6.5106 - val_acc: 0.1589\n",
    "# cost time : 3561.255692243576 second\n",
    "\n",
    "# 80% data\n",
    "# Epoch 50/50\n",
    "# 44048/44055 [============================>.] - ETA: 0s - loss: 2.4035 - acc: 0.4180Epoch 00049: val_loss did not improve\n",
    "# 44055/44055 [==============================] - 138s - loss: 2.4034 - acc: 0.4180 - val_loss: 5.9511 - val_acc: 0.1785\n",
    "# cost time : 7536.25204873085 second"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#print(len(inputs[0][0]))\n",
    "#longinputs = [i[0]+i[1] for i in inputs]\n",
    "#print(len(longinputs[0]), longinputs[0])\n",
    "DEBUG = False\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='weights3.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "#inout_list = [(\"inputs1.p\", \"outputs1.p\"), ...]\n",
    "for input_file, output_file in inout_list[:]:\n",
    "    try : model = load_model('weights3.hdf5')\n",
    "    except : pass\n",
    "    \n",
    "    inputs  = pickle.load(open(input_file, 'rb'))\n",
    "    outputs = pickle.load(open(output_file, 'rb'))\n",
    "    \n",
    "    if DEBUG : print(type(list(inputs[0][0])), type(inputs[0][0].as_matrix()))\n",
    "\n",
    "    longinputs = [list(i[0].as_matrix())+list(i[1].as_matrix()) for i in inputs] #should be, (653, 8524)\n",
    "    \n",
    "    if DEBUG : print(type(inputs[0][0]), type(outputs), len(longinputs), len(longinputs[0]), len(inputs), len(outputs))\n",
    "    \n",
    "    model.fit(np.array(longinputs), np.array(outputs), epochs=50, batch_size=16, validation_split=0.1, callbacks=[checkpointer])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16,\n",
       " 'do_validation': True,\n",
       " 'epochs': 50,\n",
       " 'metrics': ['loss', 'acc', 'val_loss', 'val_acc'],\n",
       " 'samples': 44055,\n",
       " 'steps': None,\n",
       " 'verbose': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "model.save('carroll_model80.h5')\n",
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608/653 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6102490667909795, 0.61715160942369729]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = pickle.load(open(\"inputs99.p\", 'rb'))\n",
    "outputs = pickle.load(open(\"outputs99.p\", 'rb'))\n",
    "\n",
    "longinputs = [list(i[0].as_matrix())+list(i[1].as_matrix()) for i in inputs] #should be, (653, 8524)\n",
    "\n",
    "model.evaluate(np.array(longinputs), np.array(outputs))\n",
    "# before : [8.3058816287638031, 0.099540581929555894]\n",
    "# after  : [8.4662098921093794, 0.10107197549770292]\n",
    "# weight3 : [8.4188709536518846, 0.05359877488514548]\n",
    "# weights3_e50_b16 : [1.5340424204675929, 0.62327718296605994]\n",
    "# weights80percent_e50_b16 : [1.6102490667909795, 0.61715160942369729 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(word1, word2):\n",
    "    index1 = vocabulary.index(word1)\n",
    "    index2 = vocabulary.index(word2)\n",
    "    vec1 = [0] * 4262 #len(vocabulary)\n",
    "    vec2 = [0] * 4262\n",
    "    vec1[index1] = 1\n",
    "    vec2[index2] = 1\n",
    "    return vec1 + vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 2\n",
      "94 2 8524\n",
      "94 188 1912\n",
      "False\n",
      "False\n",
      "94 1\n",
      "6328 1\n"
     ]
    }
   ],
   "source": [
    "the_book_bigram = bigram(\"<Start>\", \"alice\")\n",
    "the_book_bigram2 = bigram(\"<Start>\", \"king\")\n",
    "\n",
    "print(np.argmax(the_book_bigram), sum(the_book_bigram))\n",
    "print(np.argmax(the_book_bigram2), sum(the_book_bigram2), len(the_book_bigram2))\n",
    "\n",
    "print(vocabulary.index(\"<Start>\"), vocabulary.index(\"alice\"), vocabulary.index(\"i\"))\n",
    "ve1, ve2, ve3 = [0]*4262, [0]*4262, [0]*4262 \n",
    "ve1[94], ve2[582], ve3[1912] = 1, 1, 1\n",
    "\n",
    "print(the_book_bigram == ve1 + ve2)\n",
    "print(the_book_bigram2 == ve1 + ve3)\n",
    "\n",
    "for n, v in enumerate(the_book_bigram2):\n",
    "    if v!=0 : print(n, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "the_book_prediction = model.predict(np.array([the_book_bigram]))\n",
    "the_book_prediction2 = model.predict(np.array([the_book_bigram2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1502   20  282 3176  385 4060 3249 1763 3474 2449  582 4090 2009 3190  191\n",
      "  581  214 3486 4122 1595]\n",
      "[  20 4085  214 3315   14 1018 3176 4130  275  188 3721 2249 1775 2009    0\n",
      " 3564 3727 3731 1815  332]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "N      = 20\n",
    "skip_N = 20\n",
    "\n",
    "next_word    = np.array(the_book_prediction)\n",
    "next_word2    = np.array(the_book_prediction2)\n",
    "\n",
    "#print(next_word[0])\n",
    "argmax_top_N = next_word.argsort()[0][-(skip_N +N):-skip_N][::-1]\n",
    "argmax_top_N2 = next_word2.argsort()[0][-N:][::-1]\n",
    "\n",
    "print(argmax_top_N)\n",
    "print(argmax_top_N2)\n",
    "print(set(argmax_top_N)==set(argmax_top_N2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folded\n",
      ",\n",
      "asked\n",
      "said\n",
      "began\n",
      "waited\n",
      "seemed\n",
      "hastily\n",
      "soon\n",
      "must\n",
      "can\n",
      "watched\n",
      "is\n",
      "sat\n",
      "all\n",
      "came\n",
      "and\n",
      "sounded\n",
      "were\n",
      "gave\n"
     ]
    }
   ],
   "source": [
    "candidate_list = [vocabulary[next_word] for next_word in argmax_top_N]\n",
    "for next_word in argmax_top_N :\n",
    "    print(vocabulary[next_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "whispered\n",
      ".\n",
      "''\n",
      "but\n",
      "a\n",
      "great\n",
      "copy\n",
      "of\n",
      "it\n",
      "!\n",
      "_\n",
      "each\n",
      "project\n",
      "works\n",
      "'s\n",
      "hands\n",
      "that\n",
      "there\n",
      "king\n",
      "did\n",
      "his\n",
      "manage\n",
      "the\n",
      "table\n",
      "behind\n",
      "another\n",
      ":\n",
      "then\n",
      "they\n",
      "were\n",
      "comfortably\n",
      "mostly\n",
      "poems\n",
      "<End>\n",
      "['<Start>', 'soldiers', 'and', 'whispered', '.', \"''\", 'but', 'a', 'great', 'copy', 'of', 'it', '!', '_', 'each', 'project', 'works', \"'s\", 'hands', 'that', 'there', 'king', 'did', 'his', 'manage', 'the', 'table', 'behind', 'another', ':', 'then', 'they', 'were', 'comfortably', 'mostly', 'poems', '<End>']\n"
     ]
    }
   ],
   "source": [
    "N      = 7\n",
    "skip_N = 2\n",
    "\n",
    "LM_sentence = [\"<Start>\"]\n",
    "\n",
    "next_word = np.random.choice(vocabulary)\n",
    "LM_sentence.append(next_word)\n",
    "#LM_sentence.append(\"alice\")\n",
    "find_END  = False\n",
    "duplicate = False\n",
    "\n",
    "while not find_END:\n",
    "\n",
    "    if LM_sentence[-1] == \"<End>\" :\n",
    "        break; find_END=True;\n",
    "    try : the_book_bigram = bigram(LM_sentence[-2], LM_sentence[-1])\n",
    "    # some words don't exsit in index, e.g., 2009 is not in list\n",
    "    except : continue\n",
    "    \n",
    "    the_book_prediction = model.predict(np.array([the_book_bigram]))\n",
    "    next_word    = np.array(the_book_prediction)\n",
    "    #argmax_top_N = next_word.argsort()[0][-N:][::-1]\n",
    "    argmax_top_N = next_word.argsort()[0][-(skip_N +N):-skip_N][::-1]\n",
    "    \n",
    "    candidate_list = [vocabulary[next_word] for next_word in argmax_top_N]\n",
    "    \n",
    "    next_word = np.random.choice(candidate_list)\n",
    "    \n",
    "    #want to not save -RRB-, -LRB- stuff.\n",
    "    if next_word.startswith('-') and next_word.endswith('-') : continue\n",
    "    elif next_word in LM_sentence : continue\n",
    "    \n",
    "    LM_sentence.append(next_word)\n",
    "    \n",
    "    print(next_word)\n",
    "    if next_word == \"<End>\" :\n",
    "        find_END = True\n",
    "\n",
    "print(LM_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
